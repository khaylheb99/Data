{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of contents","metadata":{}},{"cell_type":"markdown","source":"- [Introduction](#introduction)\n- [Setup modules](#setup-modules)\n- [Load the model and check its output](#load-the-model-and-check-its-output)\n- [Measure perplexity for this model](#measure-perplexity-for-this-model)\n- [Conclusion](#conclusion)\n- [Future directions](#future-work)","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"**About the finetune** \n\nOn this Kaggle notebook: https://www.kaggle.com/code/pablomarino/deepseek-r1-distill-vs-llama-8b#Observations I tested base DeepSeek-R1-Distill-Llama-8B and compared it against Llama 8b.  ","metadata":{}},{"cell_type":"markdown","source":"# Setup modules","metadata":{}},{"cell_type":"code","source":"# need these for scorer to not fail\n!pip install --upgrade widgetsnbextension\n!pip install --upgrade ipywidgets\n!jupyter nbextension enable --py widgetsnbextension\n\n\n!pip install \"trl<0.15.0\"\n!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n!pip install -qqq --no-deps xformers==\"0.0.27.post2\" peft accelerate bitsandbytes triton==3.1.0 --progress-bar off\n\n\n\nfrom torch import __version__; from packaging.version import Version as V\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:53:36.658485Z","iopub.execute_input":"2025-02-19T23:53:36.658798Z","iopub.status.idle":"2025-02-19T23:54:31.702237Z","shell.execute_reply.started":"2025-02-19T23:53:36.658774Z","shell.execute_reply":"2025-02-19T23:54:31.701555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nimport wandb\nimport torch \nfrom kaggle_secrets import UserSecretsClient\nimport csv\nimport os\nimport gc\n\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nWANDB_TOKEN = user_secrets.get_secret(\"WANDB_TOKEN\") \nFINETUNED_MODEL_ID = \"Pablonm/FineLlama-3.1-8B_v13\"\nMAX_TOKENS = 5000\n\nwandb.login(key=WANDB_TOKEN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:54:31.703204Z","iopub.execute_input":"2025-02-19T23:54:31.703633Z","iopub.status.idle":"2025-02-19T23:55:05.763728Z","shell.execute_reply.started":"2025-02-19T23:54:31.703609Z","shell.execute_reply":"2025-02-19T23:55:05.763059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the model and check its output","metadata":{}},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=FINETUNED_MODEL_ID,\n    max_seq_length=MAX_TOKENS,\n    load_in_4bit=True,\n    dtype=None,\n)\nmodel = FastLanguageModel.for_inference(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T23:55:05.765013Z","iopub.execute_input":"2025-02-19T23:55:05.765575Z","iopub.status.idle":"2025-02-20T00:03:01.154931Z","shell.execute_reply.started":"2025-02-19T23:55:05.765551Z","shell.execute_reply":"2025-02-20T00:03:01.154261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROMPT1 = \"\"\"Rearrange the words to create the most coherent order. Think step by step and provide your answer between <answer></answer> tags.\\nInput:\\n\"\"\"\nmessages = [\n    {\"role\": \"system\", \"content\": PROMPT1},\n    {\"role\": \"user\", \"content\": \"advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\nr = model.generate(input_ids=inputs, streamer=text_streamer, use_cache=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T00:03:01.155976Z","iopub.execute_input":"2025-02-20T00:03:01.156176Z","iopub.status.idle":"2025-02-20T00:03:34.312422Z","shell.execute_reply.started":"2025-02-20T00:03:01.156158Z","shell.execute_reply":"2025-02-20T00:03:34.311779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROMPT1 = \"\"\"Rearrange the words to create the most coherent order. Think step by step and provide your answer between <answer></answer> tags.\\nInput:\\n\"\"\"\nmessages = [\n    {\"role\": \"system\", \"content\": PROMPT1},\n    {\"role\": \"user\", \"content\": \"red advent chimney elf family bless gingerbread mistletoe ornament reindeer scrooge\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\nr = model.generate(input_ids=inputs, streamer=text_streamer, use_cache=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T00:03:34.313304Z","iopub.execute_input":"2025-02-20T00:03:34.313657Z","iopub.status.idle":"2025-02-20T00:04:06.721238Z","shell.execute_reply.started":"2025-02-20T00:03:34.313624Z","shell.execute_reply":"2025-02-20T00:04:06.720599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Measure perplexity for this model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('/kaggle/input/santa-2024/sample_submission.csv')\nprint(data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T00:04:06.722085Z","iopub.execute_input":"2025-02-20T00:04:06.722391Z","iopub.status.idle":"2025-02-20T00:04:06.752989Z","shell.execute_reply.started":"2025-02-20T00:04:06.722359Z","shell.execute_reply":"2025-02-20T00:04:06.752351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_prediction(model, tokenizer, sentence, prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": sentence},\n    ]\n    inputs = tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            add_generation_prompt=True,\n            return_tensors=\"pt\",\n        ).to(\"cuda\")\n    output = model.generate(input_ids=inputs, temperature=0.0001, max_new_tokens=2000, use_cache=True)\n    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n    return decoded_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T01:31:42.515117Z","iopub.execute_input":"2025-02-20T01:31:42.515399Z","iopub.status.idle":"2025-02-20T01:31:42.520158Z","shell.execute_reply.started":"2025-02-20T01:31:42.515375Z","shell.execute_reply":"2025-02-20T01:31:42.519356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"red advent chimney elf family bless gingerbread mistletoe ornament reindeer scrooge\"\nPROMPT = \"\"\"Rearrange the words to create the most coherent order. Think step by step and provide your answer between <answer></answer> tags.\\nInput:\\n\"\"\"\n\nget_prediction(model, tokenizer, sentence, PROMPT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T01:31:53.177982Z","iopub.execute_input":"2025-02-20T01:31:53.178262Z","iopub.status.idle":"2025-02-20T01:32:22.620016Z","shell.execute_reply.started":"2025-02-20T01:31:53.178241Z","shell.execute_reply":"2025-02-20T01:32:22.619265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        self.cache = {}\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        debug : bool, default=False\n            Print debugging information.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                if text in self.cache:\n                    print(\"score cache hit\")\n                    loss_list.append(self.cache[text])\n                    print(f\"score: {self.cache[text]} for text: {text}\")\n                    continue\n                    \n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n                self.loss_fct = self.loss_fct.to(DEVICE)  # Add this after initializing loss_fct\n                \n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_item = sequence_loss.cpu().item()\n                loss_list.append(loss_item)\n                self.cache[text] = loss_item\n\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T01:14:55.179209Z","iopub.execute_input":"2025-02-20T01:14:55.179549Z","iopub.status.idle":"2025-02-20T01:14:55.194594Z","shell.execute_reply.started":"2025-02-20T01:14:55.179514Z","shell.execute_reply":"2025-02-20T01:14:55.193737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\n\ndef fix_llm_output(llm_output, original_sentence):\n    start_tag = \"<answer>\"\n    end_tag = \"</answer>\"\n    \n    try:\n        # Find the last occurrence of start tag\n        last_start_index = llm_output.rfind(start_tag)\n        \n        if last_start_index == -1:\n            print(\"Answer tags not found in LLM output(defaulting to original sentence): \", llm_output)\n            return original_sentence\n            \n        # Find the end tag that follows the last start tag\n        start_index = last_start_index + len(start_tag)\n        end_index = llm_output.find(end_tag, start_index)\n        \n        if end_index == -1:\n            print(\"Closing answer tag not found after last opening tag(defaulting to original sentence): \", llm_output)\n            return original_sentence\n            \n        answer = llm_output[start_index:end_index].strip()\n        if not answer:\n            print(\"empty answer, returning original sentence\")\n            return original_sentence\n        # Create sets of words from input and answer (case-insensitive)\n        input_words = set(word.lower() for word in original_sentence.split())\n        answer_words = answer.split()\n        \n        # Filter answer words to keep only those present in input (case-insensitive)\n        filtered_answer = []\n        for word in answer_words:\n            if word.lower() in input_words:\n                filtered_answer.append(word)\n        \n        # Join the filtered words back into a sentence\n        final_answer = ' '.join(filtered_answer)\n        return final_answer\n        \n    except Exception as e:\n        print(f\"Error extracting answer: {e}\")\n        return llm_output  # Return full output as fallback\n\ndef score_outputs(llm_outputs):\n    preds = []\n    i = 0\n    original_sentences = data.text.tolist()\n    \n    for llm_output in llm_outputs:\n        try:\n            original_sentence = original_sentences[i]\n            clean_sentence = fix_llm_output(llm_output, original_sentence)\n            preds.append(clean_sentence)\n        except (ValueError, SyntaxError, IndexError) as e:\n            # If parsing fails, use the original sentence and log the error\n            print(f\"Error parsing output at index {i}: {str(e)}\")\n            print(f\"Original LLM output: {llm_output}\")\n            print(\"Will fallback to original sentence...\")\n            llm_outputs.append(original_sentences[i])\n            preds.append(original_sentences[i])\n        \n        i += 1\n    \n    scores = scorer.get_perplexity(preds)\n    return llm_outputs, preds, np.mean(scores)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T01:44:24.855726Z","iopub.execute_input":"2025-02-20T01:44:24.856017Z","iopub.status.idle":"2025-02-20T01:44:24.864804Z","shell.execute_reply.started":"2025-02-20T01:44:24.855994Z","shell.execute_reply":"2025-02-20T01:44:24.863864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nsentences_model1 = []\nfor sentence in data.text.tolist():\n    print(\"getting prediction for sentence:\", sentence)\n    prediction = get_prediction(model, tokenizer, sentence, PROMPT)\n    sentences_model1.append(prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T01:44:28.928151Z","iopub.execute_input":"2025-02-20T01:44:28.928458Z","iopub.status.idle":"2025-02-20T01:48:58.178607Z","shell.execute_reply.started":"2025-02-20T01:44:28.928417Z","shell.execute_reply":"2025-02-20T01:48:58.177813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\nscorer = PerplexityCalculator(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T00:13:03.417905Z","iopub.execute_input":"2025-02-20T00:13:03.418254Z","iopub.status.idle":"2025-02-20T00:16:07.441989Z","shell.execute_reply.started":"2025-02-20T00:13:03.418226Z","shell.execute_reply":"2025-02-20T00:16:07.441297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputs1, answers1, score1 = score_outputs(sentences_model1)\nscore1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T01:49:11.913009Z","iopub.execute_input":"2025-02-20T01:49:11.913298Z","iopub.status.idle":"2025-02-20T01:49:16.797201Z","shell.execute_reply.started":"2025-02-20T01:49:11.913266Z","shell.execute_reply":"2025-02-20T01:49:16.796241Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion  \n\n","metadata":{}},{"cell_type":"markdown","source":"My SFT of DeepSeek-R1-Distill-Llama-8B has a perplexity score of 1443 which is much better than the best score obtained by the base model on https://www.kaggle.com/code/pablomarino/deepseek-r1-distill-vs-llama-8b of 2770. almost a 2X improvement.\n\nThat score is even lower than the one obtained using Claude Haiku: https://www.kaggle.com/code/pablomarino/prompt-optimization-on-claude-haiku\n\nThe performance is on par with the Agentic system using Claude Sonnet here: https://www.kaggle.com/code/pablomarino/claude-haiku-vs-sonnet-vs-agents\n\n","metadata":{}}]}